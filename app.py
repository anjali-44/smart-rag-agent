# -*- coding: utf-8 -*-
"""Copy of LangGraph_With_AstraDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WagSc0lZ0wYj2T9r4eQ1OOcvRUbSU0Tf
"""

# !pip install langchain langgraph cassio # helpful in initializing astradb database

import cassio
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Cassandra
from langchain.indexes.vectorstore import VectorStoreIndexWrapper
from typing import Literal
# for route query we need to inherit this base model
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_groq import ChatGroq
import os
import gradio as gr
from IPython.display import Image, display
from langgraph.graph import END, StateGraph, START
from langchain.schema import Document
from typing import List
from typing_extensions import TypedDict
from langchain_community.utilities import WikipediaAPIWrapper
from langchain_community.tools import WikipediaQueryRun

os.environ["USER_AGENT"] = "langgraph-chatbot/1.0"




## provide connection of the AstraDB
ASTRA_DB_APPLICATION_TOKEN = os.getenv("ASTRA_DB_APPLICATION_TOKEN")
ASTRA_DB_ID = os.getenv("ASTRA_DB_ID")

# to initialize
cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id = ASTRA_DB_ID)

# !pip install langchain_community

# !pip install -U tiktoken langchain-groq langchainhub langchain langgraph langchain_huggingface

# Build Index



#Docs to Index
urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm"
]

## load the url
docs = [WebBaseLoader(url).load() for url in urls]
doc_list = [item for sublist in docs for item in sublist]
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap = 0)
docs_split = text_splitter.split_documents(doc_list)

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# insert all converted vectors through embeddings techniques into a vectorDB

astra_vector_store = Cassandra(embedding = embeddings,
                               table_name = "qa_mini_demo",
                               keyspace = None
                               )
# this will get connect to my db with this embeddings and table created is table_name

#inside this table we going to store our data

astra_vector_store.add_documents(docs_split) ## insert all the documents
# print("Inserted %i headlines." % len(docs_split))
astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)

retriever = astra_vector_store.as_retriever()
## now vector db completed

## create a wikiserach by using an external tool
# before that lets define our Router Function



# Data Model
class RouteQuery(BaseModel):
  """Route a user qury to the most relevant datasource."""
  datasource: Literal["vectorstore", "wiki_search"] = Field(
      ...,
      description = "Given a user question, choose to route it to wikipedia or a vectorstore."
  )


groq_api_key = os.getenv('groq_API_key')

# initialize llm model

llm = ChatGroq(groq_api_key = groq_api_key, model_name = "llama-3.3-70b-versatile")

structure_llm_router = llm.with_structured_output(RouteQuery)

# Prompt
system = """You are an expert at routing a user question to a vecyorstore or wikipedia.
The vectorstore contains document realted to agents, prompt engineering, and adversarial attacks.
Use the vectorstore for questions on these topics. Otherwise, use wiki-serach."""

route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
question_router = route_prompt | structure_llm_router


# now integrating wikipedia

# !pip install langchain_community
# !pip install wikipedia


api_wrapper = WikipediaAPIWrapper(top_k_results = 1, doc_content_chars_max = 500)
wiki = WikipediaQueryRun(api_wrapper = api_wrapper)

# create AI agent application using langGraph


class GraphState(TypedDict):
  """
  Represents the state of our graph.

  Attributes:
      question : question
      generation : LLM generation
      documents : list of documents
  """

  question: str
  generation: str
  documents: List[str]


def retrieve(state):
  """
  Retrive documents

  Args:
      state(dict): The current graph state

  Returns:
      state(dict): New key added to state, documents, that contains retrieved documents"""

  question = state["question"]

  # Retrieval
  documents = retriever.invoke(question)
  return {"documents": documents, "question" : question}

def wiki_search(state):
  """
  wiki search based on the re-phrased question.
  Args : state(dict) : The current graph state
  Returns : state(dict) : Updates documents key with appended web results"""

  question = state["question"]

  # Wiki search
  docs = wiki.invoke({"query" : question})
  # print(docs["summary"])
  wiki_results = docs
  wiki_results = Document(page_content = wiki_results)

  return {"documents": [wiki_results], "question": question}

## Edges

def route_question(state):
  """
  Route questionto wiki search or RAG.
  Args :
      state(dict) : The current graph state

  Returns:
      str: Next node to call

  """
  print("---Route Question---")
  question = state["question"]
  source = question_router.invoke({"question": question})
  if source.datasource == "wiki_search":
    return "wiki_search"
  elif source.datasource == "vectorstore":
    print("---Route Question to RAG---")
    return "vectorstore"

def generate_answer(state):
    print("---Generate Answer---")
    documents = state["documents"]
    question = state["question"]

    # Combine docs into context string
    context = "\n\n".join(doc.page_content for doc in documents)

    # Prompt for answer
    prompt = f"Use the following context to answer the question:\n\n{context}\n\nQuestion: {question}"
    answer = llm.invoke(prompt)

    return {"question": question, "documents": documents, "generation": answer}


workflow = StateGraph(GraphState)

# define the nodes
workflow.add_node("wiki_search", wiki_search)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate_answer)  # ‚úÖ new node added

# Build the graph with conditional branching
workflow.add_conditional_edges(
    START,
    route_question,
    {
        "wiki_search": "wiki_search",
        "vectorstore": "retrieve",
    },
)

# ‚úÖ Updated edges
workflow.add_edge("retrieve", "generate")
workflow.add_edge("wiki_search", "generate")
workflow.add_edge("generate", END)

# compile
app = workflow.compile()



def chatbot_response(message, chat_history):
    try:
        final_value = None
        for output in app.stream({"question": message}):
            for key, value in output.items():
                final_value = value

        docs = final_value.get("documents", [])
        if not docs:
            return "‚ùå No relevant documents found."

        # Try vectorstore-style metadata
        try:
            description = docs[0].metadata.get("description", None)
        except Exception:
            description = None

        # Source identifier
        source_type = "üìö Vectorstore"
        if not description:
            content = getattr(docs[0], "page_content", None)
            if content:
                response = content[:800] + "..."
                source_type = "üåê Wikipedia"
            else:
                response = "‚ùå No relevant answer found."
                source_type = None
        else:
            response = description

        if source_type:
            response += f"\n\nüìå **Source**: {source_type}"

        return response

    except Exception as e:
        return f"‚ùå Error occurred: {str(e)}"

# UI
with gr.Blocks(theme=gr.themes.Soft(), css="footer {display:none !important;}") as demo:
    gr.Markdown(
        """
        <h1 style='text-align: center;'>ü§ñ LangGraph-Powered Chatbot</h1>
        <p style='text-align: center; color:gray;'>Ask anything based on your indexed documents or Wikipedia fallback</p>
        """
    )

    chatbot = gr.Chatbot(label="üì• Chat Window", height=400, bubble_full_width=False)
    with gr.Row():
        msg = gr.Textbox(placeholder="Type your question here...", show_label=False)
        clear = gr.Button("üßπ Clear")

    def respond(message, chat_history):
        response = chatbot_response(message, chat_history)
        chat_history.append((message, response))
        return "", chat_history

    def clear_chat():
        return "", []

    msg.submit(respond, [msg, chatbot], [msg, chatbot])
    clear.click(clear_chat, outputs=[msg, chatbot])

if __name__ == "__main__":
    demo.launch()

